%-----------------------------------
%
\chapter{Positional Encodings}
%
%-----------------------------------

Transformers most often incorporate positional encodings as a method for injecting information about the positions of tokens into attention operations, which do not have an inherently represention of token ordering. The original transformer model in \citet{vaswani-etal-2017-attention} used sinusoidal positional encodings, but since then, many works have proposed other encodings addressing length generalization \citep{kazemnejad2024impact}, relative distances between tokens \citep{shaw2018self}, as well as representing tree structure in sequences \citep{shiv2019}. In this section we focus on the theoretical aspects of positional encodings - how they may affect the expressiveness of the model.


\section{NoPE}

\subsection{$\frac{1}{i}$}

Obtaining the value $\frac{1}{i}$ at position $i$ can be introduced via a positional encoding, or it can be computed using a future-masked uniform attention layer \cref{sec:uniform-attention} and the presence of a beginning-of-sequence token. This positional encoding plays a role in the constructions of \citet{barcelo-etal-2024-logical}, \citet{merrill-sabharwal-2024-cot}, and others. Additionally, as described in \cref{sec:tie-breaking}, this value may be used to simulate a $\UHAT$ using an $\AHAT$.



\subsection{$\frac{i}{n}$}
Similar to above, the value $\frac{i}{n}$ may also be obtained at position $i$ using using an unmasked uniform attention layer \cref{sec:uniform-attention} and the presence of a beginning-of-sequence token. This positional encoding can be found in the constructions of \cite{merrill2023parallelism,chiang-cholak-2022-parity,strobl2024transformers}.

\section{Other Powers of $i$}

One recurring theme is using four consecutive powers of $i$, for example, $1/i^2, 1/i, 1, i$. It's not always the same powers of $i$. No idea why four are needed.


\section{Sinusoidal}

We present the sinusoidal position encodings as was introduced in~\citep{vaswani-etal-2017-attention}.
For embedding dimension \(1 \leq i \leq d\) and position \(p\), let:
\begin{align*}
    \msf{PE}(p, 2i) = \sin\left(\frac{p}{M^{2i/d}}\right),
    \quad
    \msf{PE}(p, 2i+1) = \cos\left(\frac{p}{M^{2i/d}}\right)
\end{align*}
where \(M\) is a large number, such as \(M = 10000\) in~\citep{vaswani-etal-2017-attention}.


\section{Learned Positional Embeddings}

Another option for positional encodings is to assign learnable parameter vectors to each position up to the maximum context length $n$ \citep{vaswani-etal-2017-attention}, though this strategy is rarely used in newer LMs.

From the perspective of formal language expressivity (where behavior over arbitrary-length strings matters), learned positional encodings are difficult to model because the vectors are not very uniform as a function of position, in contrast to $1/i, i/n$, or sinusoidal encodings.
Technically, when learned embeddings are used in practice, the function guaranteeing uniformity of the embeddings is the computation that trains the model, though this is typically abstracted away in theoretical analyses.
If, instead, the positional embeddings are allowed to be any function of the position, then the model is capable of expressing super-Turing computation similar to nonuniform $\mathsf{TC}^0$ circuits.

\section{Other Practical Choices}

\EF{Check what relative key, relative key query positional embeddings}

\subsection{RoPE}

\AX{I will learn RoPE is and write}

\subsection{ALIBI}

\WM{will write, note: uniform attention can be computed}