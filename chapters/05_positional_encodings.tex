%-----------------------------------
%
\chapter{Positional Encodings}
%
%-----------------------------------

Many different kinds of ingredients here.

\section{Sinusoidal}

We present the sinusoidal position encodings as was introduced in~\citep{vaswani-etal-2017-attention}.
For embedding dimension \(1 \leq i \leq d\) and position \(p\), let:
\begin{align*}
    \msf{PE}(p, 2i) = \sin\left(\frac{p}{M^{2i/d}}\right),
    \quad
    \msf{PE}(p, 2i+1) = \cos\left(\frac{p}{M^{2i/d}}\right)
\end{align*}
where \(M\) is a large number, such as \(M = 10000\) in~\citep{vaswani-etal-2017-attention}.

\subsection{Rotary Positional Embeddings (RoPE)}
\cite{su2024roformer}


\AY{Can write an intro of some sort}

% \section{Finite Image}
% \AY{Why is this here?}

\section{NoPE}

\subsection{$\frac{1}{i}$}

\AY{I can say some stuff about this}

\subsection{$\frac{i}{n}$}
\AY{Where is this used?}

\section{Other Powers of $i$}

One recurring theme is using four consecutive powers of $i$, for example, $1/i^2, 1/i, 1, i$. It's not always the same powers of $i$. No idea why four are needed.

\section{Sinusoidal}

\AX{I can put a note on MODULO. eg construction for parity - see Overcoming a Theoretical Limitation of Self Attention Chiang and Cholak}

\section{Learned Positional Embeddings}

\WM{I will write}

\section{Other Practical Choices}

\subsection{RoPE}

\subsection{ALIBI}