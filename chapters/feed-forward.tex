%-----------------------------------
%
\chapter{Feed-Forward Layers}
%
%-----------------------------------
\AY{The plan for this section before the next cooking session: Write intro, fill out various useful constructions, can be messy draft for now. Everything should be straightforward as continuous piecewise linear functions, but it is good to write out explicit constructions and how one might do them efficiently.}

\DC{Should preface this with theorems about being able to express any continuous piecewise linear function. Special cases: $\R \to \R$ can be one with one hidden layer, Boolean functions with one hidden layer. General case: number of layers is $\log(\text{number of pieces})$}

\AY{Potentially how we can structure this}\AY{Testing git}\AY{git works! }

Feed-Forward layers are an essential component of any good transformer! They allow you to compute position-wise functions etc etc etc and more background information

\AY{Other activation functions. *LU}
\AY{Unveiling chain of thought GELU---multiplication!!!}

\begin{definition}{$\ReLU$}{}
    The activation function $\ReLU : \R^d\to \R^d$, which stands for \say{Rectified Linear Unit}, is a standard component of feed-forward layers, where

    \[\ReLU\left(\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_d\\
    \end{bmatrix}\right) = \begin{bmatrix}
        \max(0,x_1)\\
        \max(0,x_2)\\
        \vdots\\
        \max(0,x_d)\\
    \end{bmatrix}\]
\end{definition}

\begin{definition}{Feed-Forward Layer}{hidden text?}
    A feed-forward network is a function $f:\R^d\to \R^d$ that computes 

    \[FFN\left(\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_d\\
    \end{bmatrix}\right)= L_2\left(\ReLU\left(L_1\left(\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_d\\
    \end{bmatrix}\right)\right)\right)\]

    Where $L_1: \R^d \to \R^{d_{ff}}$ and $L_2: \R^{d_{ff}} \to \R^{d}$ are affine transformations. $L_1$ uses linear transformation $W_1:\R^d\to \R^{d_{ff}}$ and bias $b_1\in\R^{d_{ff}}$

    \[L_1\left(\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_d\\
    \end{bmatrix}\right)=W_1\left(\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_{d}\\
    \end{bmatrix}\right)+b_1\]

    And $L_2$ uses linear transformation $W_2:\R^{d_{ff}}\to \R^d$ and bias $b_2\in\R^{d}$
    \[L_2\left(\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_d\\    \end{bmatrix}\right)=W_2\left(\ReLU\left(\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_{d_{ff}}\\
    \end{bmatrix}\right)\right)+b_2\]
\end{definition}

A two-layer feed-forward network with ReLU activations on the first layer and none on the second layer can compute any continuous piecewise linear function with a finite number of pieces, or CPWL \citep{arora+:2018}. Here we show some common recipes using feed-forward networks. 

\subsection{Min and Max}
    Recall that $\ReLU(x)=\max(0,x)$. Then  $\min(x,y) = x-\ReLU(x-y)$. 
    % $= y-\ReLU(y-x)$. To see this, there are two cases:
    \begin{itemize}
        \item If $x<y$, then $\ReLU(x-y)=0$, so $x-\ReLU(x-y)=x = \min(x,y)$.
        \item If $x\geq y$, then $\ReLU(x-y)=x-y$, so $x-\ReLU(x-y)=x-(x-y)=y = \min(x,y)$.
    \end{itemize}
    Similarly, $\max(x,y) = x+\ReLU(y-x)$. Therefore there exist FFNs to compute the min or max of two real numbers:
    \begin{center}
    \begin{tabular}{c@{\hspace*{4em}}c}
      \begin{tikzpicture}[x=1.5cm,y=1.5cm,baseline=1cm]
        \node (x1) at (0,0) [input,label=below:{$x$}];
        \node (x2) at (1,0) [input,label=below:{$y$}];
        \node (h1) at (-0.5,1) [relu] edge node[near start] {$1$} (x1);
        \node (h2) at (0.5,1) [relu] edge node {$-1$} (x1);
        \node (h3) at (1.5,1) [relu] edge node[near start] {$1$} (x1) edge node[auto=left,near start] {$-1$} (x2);
        \node (y) at (0.5,2) [output,label=above:{$\min(x,y)$}] edge node {$1$} (h1) edge node[auto=left] {$-1$} (h2) edge node[auto=left] {$-1$} (h3);
      \end{tikzpicture} &
      \begin{tikzpicture}[x=1.5cm,y=1.5cm,baseline=1cm]
        \node (x1) at (0,0) [input,label=below:{$x$}];
        \node (x2) at (1,0) [input,label=below:{$y$}];
        \node (h1) at (-0.5,1) [relu] edge node[near start] {$1$} (x1);
        \node (h2) at (0.5,1) [relu] edge node {$-1$} (x1);
        \node (h3) at (1.5,1) [relu] edge node[near start] {$-1$} (x1) edge node[auto=left,near start] {$1$} (x2);
        \node (y) at (0.5,2) [output,label=above:{$\max(x,y)$}] edge node {$1$} (h1) edge node[auto=left] {$-1$} (h2) edge node[auto=left] {$1$} (h3);
      \end{tikzpicture}
    \end{tabular}
    \end{center}    

    \subsection{Boolean Functions}
    \AY{Need some note on Boolean representations}
    The Boolean operations $\land$ and $\lnot$ can be computed by FFNNs with $\ReLU$ activations. Conjunction ($\land$) is equivalent to min/max:
  \begin{align*}
  \begin{bmatrix} \vdots \\ -2B_1+1 \\ \phantom{-}2B_1-1 \\ \vdots \end{bmatrix} \land \begin{bmatrix} \vdots \\ -2B_2+1 \\ \phantom{-}2B_2-1 \\ \vdots \end{bmatrix} &= \begin{bmatrix} \vdots \\ \max(-2B_1+1,-2B_2+1) \\ \min(\phantom{-}2B_1-1,\phantom{-}2B_2-1) \\ \vdots \end{bmatrix}.
  \end{align*}

  Logical negation ($\lnot$) is equivalent to arithmetic negation, or swapping the positive and negative components:
  \begin{center}
      \begin{tikzpicture}[x=2cm,y=1.5cm,baseline=1cm]
        \node (x1) at (0,0) [input,label=below:{$2B-1$}];
        \node (x2) at (1,0) [input,label=below:{$-2B+1$}];
        \node (h1) at (0,1) [relu] edge node {$1$} (x1);
        \node (h2) at (1,1) [relu] edge node {$1$} (x2);
        \node (y1) at (0,2) [output] edge node {$1$} (h1) edge node[near start] {$-1$} (h2);
        \node (y2) at (1,2) [output] edge node[auto=left,near start] {$-1$} (h1) edge node[auto=left] {$1$} (h2);
      \end{tikzpicture}
  \end{center}

    For an arbitrary Boolean formula, convert it to \emph{canonical} disjunctive normal form, which is a disjunction $\phi_1 \lor \cdots \lor \phi_n$ of clauses, at most one of which can be true for any value of the inputs. \AY{wait what was going on here? I forget}

    Each clause is of the form $\phi_m = \bigwedge_{k\in K_m} \psi_k$, where each $\phi_k$ is an input or a negated input and $K_m$ is a set of indices for each clause. A slightly different construction can be used to compute $\land$ over inputs in $K_m$. Observe that:

    \begin{align*}
    \bigwedge_{k\in K_m} B_k &= \ReLU\left(\left(\sum_{k\in K_m} (B_k)\right) -(|K_m|-1)\right) \\ &=\ReLU\left(\left(\sum_{k\in K_m} \frac{1}{2}(2B_k-1) \right) - \frac{3|K_m|}{2}+1)\right).
    \end{align*}
    
    And this can be computed for each clause using the first layer and $\ReLU$ of a feed-forward layer. Recall again that if the constant $m$ is fixed, we can retrieve it by multiplying the constant $\pm 1$ from dimensions $2k_0-1,2k_0$ as described in \AY{Somewhere}. Then, because at most one clause can be true, the sum of all clauses will either be $1$ or $0$. Then, we convert back to the $\pm1$ representation of truth values. 

    \[\bigvee_{m=1}^n \left(\bigwedge_{k\in K_m} B_k \right)=2\cdot\left(\sum_{m=1}^n\ReLU\left(\left(\sum_{k\in K_m} \frac{1}{2}(2B_k-1) \right) - \frac{3|K_m|}{2}+1)\right)\right)-1.\]

    This can all be done in a single feed-forward layer.
