@inproceedings{vaswani-etal-2017-attention,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All you Need},
  booktitle = {Advances in Neural Information Processing Systems 30 (NeurIPS 2017)},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{arora+:2018,
  author = "Raman Arora and Amitabh Basu and Poorya Mianjy and Anirbit Mukherjee",
  title = "Understanding Deep Neural Networks with Rectified Linear Units",
  year = 2018,
  booktitle = "Proceedings of ICLR",
  url = "https://openreview.net/forum?id=B1J_rgWRW",
}

@inproceedings{merrill2024the,
    title={The Expressive Power of Transformers with Chain of Thought},
    author={William Merrill and Ashish Sabharwal},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=NjNGlPh8Wh}
}

@article{feng2024towards,
  title={Towards revealing the mystery behind chain of thought: a theoretical perspective},
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@misc{hendrycks2023,
    title = {Gaussian Error Linear Units (GELUs)},
    author = {Dan Hendrycks and Kevin Gimpel},
    year = {2023},
    eprint = {1606.08415},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    doi = {10.48550/arXiv.1606.08415},
}

@article{Fukushima1975,
    author = {Fukushima, Kunihiko},
    title = {Cognitron: A self-organizing multilayered neural network},
    journal = {Biological Cybernetics},
    year = {1975},
    month = {Sep},
    day = {01},
    volume = {20},
    number = {3},
    pages = {121-136},
    abstract = {A new hypothesis for the organization of synapses between
                neurons is proposed: ``The synapse from neuron x to neuron y is
                reinforced when x fires provided that no neuron in the vicinity
                of y is firing stronger than y''. By introducing this hypothesis,
                a new algorithm with which a multilayered neural network is
                effectively organized can be deduced. A self-organizing
                multilayered neural network, which is named ``cognitron'', is
                constructed following this algorithm, and is simulated on a
                digital computer. Unlike the organization of a usual brain models
                such as a three-layered perceptron, the self-organization of a
                cognitron progresses favorably without having a ``teacher'' which
                instructs in all particulars how the individual cells respond.
                After repetitive presentations of several stimulus patterns, the
                cognitron is self-organized in such a way that the receptive
                fields of the cells become relatively larger in a deeper layer.
                Each cell in the final layer integrates the information from
                whole parts of the first layer and selectively responds to a
                specific stimulus pattern or a feature.},
    issn = {1432-0770},
    doi = {10.1007/BF00342633},
    url = {https://doi.org/10.1007/BF00342633}
}
