@inproceedings{vaswani-etal-2017-attention,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30 (NeurIPS)},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
}

@inproceedings{arora+:2018,
  author = "Raman Arora and Amitabh Basu and Poorya Mianjy and Anirbit Mukherjee",
  title = "Understanding Deep Neural Networks with Rectified Linear Units",
  year = 2018,
  booktitle = "Proceedings of the Sixth International Conference on Learning Representations (ICLR)",
  url = "https://openreview.net/forum?id=B1J_rgWRW",
}

@inproceedings{merrill2024the,
    title={The Expressive Power of Transformers with Chain of Thought},
    author={William Merrill and Ashish Sabharwal},
    booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
    year={2024},
    url={https://openreview.net/forum?id=NjNGlPh8Wh}
}

@inproceedings{feng2024towards,
  title = "Towards Revealing the Mystery behind {C}hain of {T}hought: A Theoretical Perspective",
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  booktitle={Advances in Neural Information Processing Systems 36 (NeurIPS)},
  year={2023},
  url = "https://papers.nips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html",
}

@inproceedings{akyurek2022learning,
  title={What learning algorithm is in-context learning? {I}nvestigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle = "The Eleventh International Conference on Learning Representations (ICLR)",
  year={2023},
  url = "https://openreview.net/forum?id=0g0X4H8yN4I",
}

@misc{hendrycks2023,
    title = {Gaussian Error Linear Units (GELUs)},
    author = {Dan Hendrycks and Kevin Gimpel},
    year = {2023},
    eprint = {1606.08415},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    doi = {10.48550/arXiv.1606.08415},
}

@article{Fukushima1975,
    author = {Fukushima, Kunihiko},
    title = {Cognitron: A self-organizing multilayered neural network},
    journal = {Biological Cybernetics},
    year = {1975},
    month = {Sep},
    day = {01},
    volume = {20},
    number = {3},
    pages = {121-136},
    abstract = {A new hypothesis for the organization of synapses between
                neurons is proposed: ``The synapse from neuron x to neuron y is
                reinforced when x fires provided that no neuron in the vicinity
                of y is firing stronger than y''. By introducing this hypothesis,
                a new algorithm with which a multilayered neural network is
                effectively organized can be deduced. A self-organizing
                multilayered neural network, which is named ``cognitron'', is
                constructed following this algorithm, and is simulated on a
                digital computer. Unlike the organization of a usual brain models
                such as a three-layered perceptron, the self-organization of a
                cognitron progresses favorably without having a ``teacher'' which
                instructs in all particulars how the individual cells respond.
                After repetitive presentations of several stimulus patterns, the
                cognitron is self-organized in such a way that the receptive
                fields of the cells become relatively larger in a deeper layer.
                Each cell in the final layer integrates the information from
                whole parts of the first layer and selectively responds to a
                specific stimulus pattern or a feature.},
    issn = {1432-0770},
    doi = {10.1007/BF00342633},
    url = {https://doi.org/10.1007/BF00342633}
}

@inproceedings{chiang-cholak-2022-parity,
    title = "Overcoming a Theoretical Limitation of Self-Attention",
    author = "Chiang, David  and
      Cholak, Peter",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)",
    month = may,
    year = "2022",
    xaddress = "Dublin, Ireland",
    xpublisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.527",
    doi = "10.18653/v1/2022.acl-long.527",
    pages = "7654--7664",
}

@inproceedings { merrill-sabharwal-2024-cot,
  author = "William Merrill and Ashish Sabharwal",
  year = 2024,
  title = "The Expressive Power of Transformers with Chain of Thought",
  booktitle = "Proceedings of the Twelfth International Conference on Learning Representations (ICLR)",
  arxiv_url = "https://arxiv.org/abs/2310.07923",
  url = "https://openreview.net/forum?id=NjNGlPh8Wh",
  pdf = "https://openreview.net/pdf?id=NjNGlPh8Wh",
}

@inproceedings{barcelo-etal-2024-logical,
  title = "Logical Languages Accepted by Transformer Encoders with Hard Attention",
  author = "Pablo Barcel{\'o} and Alexander Kozachinskiy and Anthony Widjaja Lin and Vladimir Podolskii",
  year = 2024,
  booktitle = "Proceedings of the Twelfth International Conference on Learning Representations (ICLR)",
  arxiv_url = "https://arxiv.org/abs/2310.03817",
  url = "https://openreview.net/forum?id=gbrHZq07mq",
  pdf = "https://openreview.net/pdf?id=gbrHZq07mq",
}

% Note that the version below has an error that results in omitting the proof that Majority can be recognized by a Transformer
@article{perez-etal-2021-turing,
  author    = {Jorge P{\'{e}}rez and
               Pablo Barcel{\'{o}} and
               Javier Marinkovic},
  title     = {Attention is {T}uring-Complete},
  journal   = {Journal of Machine Learning Research},
  volume    = {22},
  pages     = {75:1--75:35},
  year      = {2021},
  url       = {http://jmlr.org/papers/v22/20-302.html},
  timestamp = {Mon, 31 Jan 2022 17:23:36 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/PerezBM21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
